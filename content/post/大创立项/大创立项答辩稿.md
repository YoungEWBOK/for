---
title: 被Diss的立项答辩
date: '2024-05-24'
summary: 又委屈又不服气，却也觉得确实如此。自身的主观条件并不能影响评判标准，很多事情就是只看结果，而不看你有没有苦衷。
tags:
  - Memory
share: false
---

## **内容**

各位老师好，我本次立项的主题是“基于具身对话的多模态融合研究”。

这个选题源于我们自身的思考。因为协同定位在应急救援等场景中扮演着关键角色，无论是通过无人机还是救援机器人,定位目标对象都是一个核心环节。我们探索了将具身智能与协同定位相结合的可能性,由此产生了"具身对话定位"的研究方向。

具身对话定位是具身智能里的一个小分支，属于具身问答的衍生。具身问答是在3D环境中随机生成一个智能体并提出一个问题，智能体通过探索环境，来找到问题的答案。

而具身对话则是拓展成为了两个智能体。它们只能拥有各自的视角,并通过对话交互来进行协同定位的任务。一个智能体Observer随机生成在3D环境中，根据自身观察反馈环境信息；另一个智能体Locator拥有该环境的俯视视角，需要根据Observer的描述来推测并指示方向,最终定位Observer的位置。这模拟了现实中协同定位的场景。

总的来说，该领域研究相对较新,数据集规模有限,任务综合性强、复杂度较高。

针对现状，我们提出项目的一些特色和创新点。由于现有工作主要集中在室内场景,模型泛化能力较弱，我们希望将场景扩展至更广阔的应用场景,并探索多模态信息的融合,以增强模型的鲁棒性和泛化能力。

同时,我们计划结合多模态大模型,提高语言理解和生成的质量,从而增强人机交互的自然性,让我们的模型能扩展到其他的应用场景，例如为视障人士提供辅助导航等。

接下来是我们项目的总体规划。

目前我们已经完成了初期阶段的工作，进入了中期分析、改善模型的阶段。

以下是我们的初期进展。当前,我们已完成相关论文阅读、并做好了整理；针对存在问题的数据集和代码，我们进行了逐一完善。

同时，在模型训练过程中我们也发现了一些问题,如：复现得到的结果同论文给出的数据有一定的差距；以及原有模型采用KL散度的损失函数，使得训练误差出现负值等。我们已经改进了损失函数的设计，解决了这一问题。在此之上，我们对语言模型进行了改进，将原先的RNN改成了Bert模型；同时我们加深了残差网络的层数，提取更丰富的图像特征；当前我们正引入自注意力机制模块，用于学习句子内部的词依赖关系，捕获句子的内部结构，也用于获取特征图的全局空间信息等，并以此来改善模型性能。

未来,我们计划持续优化模型,消除现有问题,并最终形成完整的论文项目。希望能够得到各位老师的宝贵指导和建议,使我们的工作更加完善。

谢谢！

## **一些可能的问题**

**ResNet**

ResNet18和ResNet50的主要区别在于网络深度和宽度。ResNet18包含18个卷积层，而ResNet50包含50个卷积层。此外，ResNet50使用了更多的瓶颈层（Bottleneck layer），这种层结构可以在增加网络深度的同时减少计算量。

瓶颈层由三个卷积层组成：1x1卷积层用于降维，3x3卷积层用于主要特征提取，另一个1x1卷积层用于升维。这种结构可以在保证特征提取能力的同时，减少网络参数和计算量。

KL散度(Kullback-Leibler Divergence)是一种常用于衡量两个概率分布之间的差异或距离的非对称性度量。它广泛应用于很多机器学习和深度学习中作为损失函数或正则化项,例如在生成对抗网络(GAN)、变分自编码器(VAE)等模型中。

**KLDivLoss**

KL散度损失函数的定义为:

{{< math >}}$KL(P||Q) = \sum_x P(x)log\frac{P(x)}{Q(x)}${{< /math >}}

其中P和Q是两个概率分布。 {{< math >}}$KL(P||Q)${{< /math >}} 表示用Q来近似P的代价或信息损失。

一些KL散度损失函数在机器学习中的应用:

1. VAE模型中,通常将 {{< math >}}$KL(q(z|x)||p(z))${{< /math >}} 加入损失函数,作为对隐变量z的先验 {{< math >}}$p(z)${{< /math >}} 的约束,防止 {{< math >}}$q(z|x)${{< /math >}} 偏离先验过多。

2. 在神经机器翻译等序列生成任务中,将解码器生成的单词概率分布与真实单词的one-hot分布的KL散度作为损失。

3. 在GAN中,通常在生成模型G的训练目标中加入 {{< math >}}$KL(P\_g||P\_data)${{< /math >}} 项,使G生成的数据分布 {{< math >}}$P\_g${{< /math >}} 尽可能逼近真实数据分布 {{< math >}}$P\_data${{< /math >}} 。

需要注意的是KL散度具有非对称性,即 {{< math >}}$KL(P||Q)${{< /math >}} 不等于 {{< math >}}$KL(Q||P)${{< /math >}} ,应根据实际任务选择合适的KL方向。另外由于KL散度涉及对数运算,当 {{< math >}}$Q(x)=0${{< /math >}} 而 {{< math >}}$P(x)${{< /math >}} 非0时,会出现无穷大的情况,因此在实际操作中常常需要采取一些平滑处理。

总的来说,KL散度提供了一种衡量概率分布差异的标准,在概率密度估计、稀疏建模等任务中发挥着重要作用。

**模型架构**

1. 使用ResNet50作为视觉编码器提取图像特征。可以选择是否冻结ResNet的参数。
2. 使用BERT作为语言编码器提取文本序列的特征表示。
3. 视觉和语言特征通过一个多层LingUNet模块进行融合:
   - 将语言特征分成多个slice,每个slice与图像特征做卷积运算得到一个门控G
   - 图像特征通过一系列卷积和上采样层,并与门控G进行残差连接
   - 最终输出是一个与输入图像同尺寸的特征图
4. 一个线性投影层将最终特征图映射为所需输出,如分类/回归分数等

**训练过程**

1. 输入是一批图像及其对应的文本序列(轮循对话)
2. 将图像通过ResNet编码,文本通过BERT编码得到视觉和语言特征
3. 将视觉和语言特征输入LingUNet模块进行多模态融合
4. 使用所需任务的损失函数(如交叉熵、MSE等)计算输出与真实标签的损失
5. 反向传播,更新整个模型(包括ResNet、BERT、LingUNet)的可训练参数

该模型的主要创新点是提出了LingUNet模块,能够较好地融合视觉和语言特征,充分利用两种模态信息。卷积门控机制允许语言特征以动态的方式控制对图像特征的加权和修改。

总的来说,这是一种端到端的多模态学习框架,能够同时学习视觉和语言表示,并在融合后的综合特征上完成下游任务,如图像理解、视觉问答等。通过预训练的ResNet和BERT模型初始化,有助于加速收敛和提高性能。

**SelfAttention**

在深度学习模型中,注意力机制是一种有效捕捉长期依赖关系的方法。自注意力机制(Self-Attention)是注意力机制的一种形式,它允许模型关注输入序列中的不同位置,以更好地编码序列的表示。

自注意力机制是这样工作的:

1) 首先,将输入序列 {{< math >}}$X = (x_1, x_2, ..., x_n)${{< /math >}} 线性映射到查询(Query)、键(Key)和值(Value)矩阵:

   {{< math >}}$Q = XW^Q${{< /math >}}
   
   {{< math >}}$K = XW^K${{< /math >}}
   
   {{< math >}}$V = XW^V${{< /math >}}

其中 {{< math >}}$W^Q, W^K, W^V${{< /math >}} 是可训练的权重矩阵。

2) 计算查询Q与所有键K的点积,获得未缩放的分数矩阵:

   {{< math >}}$scores = QK^T${{< /math >}}

3) 对分数矩阵进行缩放处理以缓解较深层中的梯度问题:

   {{< math >}}$scores' = scores / sqrt(d_k)${{< /math >}}

其中 {{< math >}}$d_k${{< /math >}} 是键的维度。

4) 对分数矩阵执行 softmax 运算,得到注意力权重矩阵:

   {{< math >}}$attention\_weights = softmax(scores')${{< /math >}}

5) 将注意力权重与值矩阵 V 相乘,得到输出表示:

   {{< math >}}$output = attention\_weights * V${{< /math >}}

可以看出,通过自注意力,模型可以自动学习到输入序列中不同位置之间的关系,并据此计算注意力权重。

要在现有模型(如LingUNet)中添加自注意力机制,可以遵循以下步骤:

1. 在适当的位置(如卷积层之后)添加一个自注意力子层
2. 实现上述自注意力计算过程
3. 将自注意力的输出与原始输入进行残差连接
4. 可以进一步添加层归一化操作以稳定训练

添加自注意力机制可以增强模型对长期依赖关系的建模能力,尤其对于处理较长序列任务很有帮助。同时也增加了一些计算开销,需要权衡模型大小与性能。总的来说,自注意力是一种有效的注意力机制,被广泛应用于 Transformer、BERT 等模型中。

**val-seen&val-unseen**

val-seen包含了在训练过程中遇到过的环境的新起始位置。val-unseen和test都包含新的环境。

为了在验证过程中分别评估模型对于新对话框和新环境的泛化能力。