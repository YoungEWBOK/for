---
title: 被Diss的立项答辩
date: '2024-05-24'
summary: 又委屈又不服气，又觉得确实如此。自身的主观条件并不能影响评判标准，很多事情就是只看结果，而不看你有没有苦衷。
tags:
  - Memory
share: false
---

# 大创立项答辩稿

## 内容

各位老师好，我本次立项的主题是“基于具身对话的多模态融合研究”。

这个选题源于我们自身的思考。因为协同定位在应急救援等场景中扮演着关键角色，无论是通过无人机还是救援机器人,定位目标对象都是一个核心环节。我们探索了将具身智能与协同定位相结合的可能性,由此产生了"具身对话定位"的研究方向。

具身对话定位是具身智能里的一个小分支，属于具身问答的衍生。具身问答是在3D环境中随机生成一个智能体并提出一个问题，智能体通过探索环境，来找到问题的答案。

而具身对话则是拓展成为了两个智能体。它们只能拥有各自的视角,并通过对话交互来进行协同定位的任务。一个智能体Observer随机生成在3D环境中，根据自身观察反馈环境信息；另一个智能体Locator拥有该环境的俯视视角，需要根据Observer的描述来推测并指示方向,最终定位Observer的位置。这模拟了现实中协同定位的场景。

总的来说，该领域研究相对较新,数据集规模有限,任务综合性强、复杂度较高。

针对现状，我们提出项目的一些特色和创新点。由于现有工作主要集中在室内场景,模型泛化能力较弱，我们希望将场景扩展至更广阔的应用场景,并探索多模态信息的融合,以增强模型的鲁棒性和泛化能力。

同时,我们计划结合多模态大模型,提高语言理解和生成的质量,从而增强人机交互的自然性,让我们的模型能扩展到其他的应用场景，例如为视障人士提供辅助导航等。

接下来是我们项目的总体规划。

目前我们已经完成了初期阶段的工作，进入了中期分析、改善模型的阶段。

以下是我们的初期进展。当前,我们已完成相关论文阅读、并做好了整理；针对存在问题的数据集和代码，我们进行了逐一完善。

同时，在模型训练过程中我们也发现了一些问题,如：复现得到的结果同论文给出的数据有一定的差距；以及原有模型采用KL散度的损失函数，使得训练误差出现负值等。我们已经改进了损失函数的设计，解决了这一问题。在此之上，我们对语言模型进行了改进，将原先的RNN改成了Bert模型；同时我们加深了残差网络的层数，提取更丰富的图像特征；当前我们正引入自注意力机制模块，用于学习句子内部的词依赖关系，捕获句子的内部结构，也用于获取特征图的全局空间信息等，并以此来改善模型性能。

未来,我们计划持续优化模型,消除现有问题,并最终形成完整的论文项目。希望能够得到各位老师的宝贵指导和建议,使我们的工作更加完善。

谢谢！

 {{< math >}}$\nabla F(\mathbf{x}_{n})${{< /math >}}`